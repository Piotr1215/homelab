---
# Source: retina-hubble/templates/agent/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: retina-agent
  namespace: kube-system
---
# Source: retina-hubble/templates/hubble-relay/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "hubble-relay"
  namespace: kube-system
---
# Source: retina-hubble/templates/hubble-ui/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "hubble-ui"
  namespace: kube-system
---
# Source: retina-hubble/templates/operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: serviceaccount
    app.kubernetes.io/instance: retina-operator
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: operator
    app.kubernetes.io/part-of: operator
    app.kubernetes.io/managed-by: kustomize
  name: retina-operator
  namespace: kube-system
---
# Source: retina-hubble/templates/agent/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: retina-config
  namespace: kube-system
data:
  leader-election: "false"
  cluster-name: default
  # Enable Hubble gRPC service.
  enable-hubble: "true"
  # UNIX domain socket for Hubble server to listen to.
  hubble-socket-path: "/var/run/cilium/hubble.sock"
  # Buffer size of the channel for Hubble to receive monitor events. If this field is not set,
  # the buffer size is set to the default monitor queue size.
  hubble-event-queue-size: "16383"
  # Capacity of the buffer to store recent events.
  hubble-event-buffer-capacity: "4095"
  # Address to expose Hubble metrics (e.g. ":7070"). Metrics server will be disabled if this
  # field is not set.
  hubble-metrics-server: ":9965"
  # A space separated list of metrics to enable. See [0] for available metrics.
  #
  # https://github.com/cilium/hubble/blob/master/Documentation/metrics.md
  hubble-metrics:
    flow:sourceEgressContext=pod;destinationIngressContext=pod
    tcp:sourceEgressContext=pod;destinationIngressContext=pod
    dns:query;sourceEgressContext=pod;destinationIngressContext=pod
    drop:sourceEgressContext=pod;destinationIngressContext=pod
  enable-hubble-open-metrics: "false"
  hubble-export-file-max-size-mb: "10"
  hubble-export-file-max-backups: "5"
  # An additional address for Hubble server to listen to (e.g. ":4244").
  hubble-listen-address: ":4244"
  hubble-disable-tls: "true"
  config.yaml: |-
    apiServer:
      host: 0.0.0.0
      port: 10093
    logLevel: info
    enabledPlugin: ["linuxutil","packetforward","packetparser","dns", "dropreason"]
    metricsInterval: 
    metricsIntervalDuration: 10s
    enableTelemetry: false
    enablePodLevel: true
    enableConntrackMetrics:
    remoteContext: true
    enableAnnotations: true
    bypassLookupIPOfInterest: true
    dataAggregationLevel: high
    monitorSockPath: /var/run/cilium/monitor1_2.sock
---
# Source: retina-hubble/templates/hubble-relay/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hubble-relay-config
  namespace: kube-system
data:
  config.yaml: |
    cluster-name: default
    peer-service: "hubble-peer.kube-system.svc.cluster.local:80"
    listen-address: :4245
    gops: true
    gops-port: "9893"
    dial-timeout: 
    retry-timeout: 
    sort-buffer-len-max: 
    sort-buffer-drain-timeout: 
    disable-client-tls: true
    disable-server-tls: true
---
# Source: retina-hubble/templates/hubble-ui/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hubble-ui-nginx
  namespace: kube-system
data:
  nginx.conf: "server {\n    listen       8081;\n    listen       [::]:8081;\n    server_name  localhost;\n    root /app;\n    index index.html;\n    client_max_body_size 1G;\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n\n        # CORS\n        add_header Access-Control-Allow-Methods \"GET, POST, PUT, HEAD, DELETE, OPTIONS\";\n        add_header Access-Control-Allow-Origin *;\n        add_header Access-Control-Max-Age 1728000;\n        add_header Access-Control-Expose-Headers content-length,grpc-status,grpc-message;\n        add_header Access-Control-Allow-Headers range,keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout;\n        if ($request_method = OPTIONS) {\n            return 204;\n        }\n        # /CORS\n\n        location /api {\n            proxy_http_version 1.1;\n            proxy_pass_request_headers on;\n            proxy_hide_header Access-Control-Allow-Origin;\n            proxy_pass http://127.0.0.1:8090;\n        }\n        location / {\n            # double `/index.html` is required here \n            try_files $uri $uri/ /index.html /index.html;\n        }\n\n        # Liveness probe\n        location /healthz {\n            access_log off;\n            add_header Content-Type text/plain;\n            return 200 'ok';\n        }\n    }\n}"
---
# Source: retina-hubble/templates/operator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: retina-operator-config
  namespace: kube-system
data:
  enable-telemetry: "false"
  log-opt: "{\"level\":\"info\"}"
  leader-election: "true"
  identity-gc-interval: 15m
  cilium-endpoint-gc-interval: 5m
---
# Source: retina-hubble/templates/agent/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: kube-system
  name: retina-cluster-reader
rules:
  - apiGroups: [""] # "" indicates the core API group
    resources: ["pods", "services", "replicationcontrollers", "nodes", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["networking.azure.com"]
    resources: ["clusterobservers"]
    verbs: ["get", "list", "watch"]
  - apiGroups:
      - retina.io
    resources:
      - retinaendpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
    - cilium.io
    resources:
    - ciliumnodes
    - ciliumidentities
    - ciliumendpoints
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - retina.io
    resources:
      - retinaendpoints
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - retina.io
    resources:
      - metricsconfigurations
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - retina.io
    resources:
      - retinaendpoints/finalizers
    verbs:
      - update
  - apiGroups:
      - retina.io
    resources:
      - retinaendpoints/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
    - discovery.k8s.io
    resources:
    - endpointslices
    verbs:
    - get
    - list
    - watch
---
# Source: retina-hubble/templates/hubble-ui/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hubble-ui
  labels:
    app.kubernetes.io/part-of: cilium
rules:
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - componentstatuses
  - endpoints
  - namespaces
  - nodes
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - cilium.io
  resources:
  - "*"
  verbs:
  - get
  - list
  - watch
---
# Source: retina-hubble/templates/operator/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: retina-operator-role
rules:
  - apiGroups:
      - "apiextensions.k8s.io"
    resources:
      - "customresourcedefinitions"
    verbs:
      - "create"
  - apiGroups:
      - "apiextensions.k8s.io"
    resources:
      - "customresourcedefinitions"
    resourceNames:
      - ciliumidentities.cilium.io
      - ciliumendpoints.cilium.io
    verbs:
      - "get"
      - "update"
      - "delete"
      - "patch"
  - apiGroups:
    - ""
    resources:
      - pods
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
    - jobs
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
      - batch
    resources:
    - jobs/status
    verbs:
    - get
  - apiGroups:
    - retina.io
    resources:
    - captures
    verbs:
    - create
    - delete
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
      - retina.io
    resources:
    - captures/finalizers
    verbs:
    - update
  - apiGroups:
      - retina.io
    resources:
    - captures/status
    verbs:
    - get
    - patch
    - update
  - apiGroups:
    - cilium.io
    resources:
    - ciliumidentities
    - ciliumendpoints
    verbs:
    - create
    - delete
    - get
    - list
    - patch
    - update
    - watch
  # for endpointgc
  # will not create ciliumnode objects
  - apiGroups:
    - cilium.io
    resources:
    - ciliumnodes
    verbs:
    - get
    - list
    - watch
  # For cilium-operator running in HA mode.
  #
  # Cilium operator running in HA mode requires the use of ResourceLock for Leader Election
  # between multiple running instances.
  # The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less
  # common and fewer objects in the cluster watch "all Leases".
  - apiGroups:
    - coordination.k8s.io
    resources:
    - leases
    verbs:
    - create
    - get
    - update
---
# Source: retina-hubble/templates/agent/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: retina-cluster-reader-binding
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: retina-agent
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: retina-cluster-reader
  apiGroup: rbac.authorization.k8s.io
---
# Source: retina-hubble/templates/hubble-ui/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hubble-ui
  labels:
    app.kubernetes.io/part-of: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hubble-ui
subjects:
- kind: ServiceAccount
  name: "hubble-ui"
  namespace: kube-system
---
# Source: retina-hubble/templates/operator/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/instance: retina-operator-rolebinding
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: operator
    app.kubernetes.io/part-of: operator
    app.kubernetes.io/managed-by: kustomize
  name: retina-operator-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: retina-operator-role
subjects:
- kind: ServiceAccount
  name: retina-operator
  namespace: kube-system
---
# Source: retina-hubble/templates/agent/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: retina-svc
  namespace: kube-system
  labels:
    app: retina
spec:
  ports:
    - port: 10093
      targetPort: 10093
  selector:
    app: retina
---
# Source: retina-hubble/templates/hubble-relay/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: hubble-relay
  namespace: kube-system
  labels:
    k8s-app: hubble-relay
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: retina
spec:
  type: "ClusterIP"
  selector:
    k8s-app: hubble-relay
  ports:
  - protocol: TCP
    port: 80
    targetPort: 4245
---
# Source: retina-hubble/templates/hubble-ui/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: hubble-ui
  namespace: kube-system
  labels:
    k8s-app: hubble-ui
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
spec:
  type: "ClusterIP"
  selector:
    k8s-app: hubble-ui
  ports:
    - name: http
      port: 80
      targetPort: 8081
---
# Source: retina-hubble/templates/hubble/metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: network-observability
  namespace: kube-system
  labels:
    k8s-app: networkobservability
    app.kubernetes.io/name: networkobservability
    app.kubernetes.io/part-of: retina
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9965"
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: hubble
    port: 9965
    protocol: TCP
    targetPort: 9965
  - name: retina
    port: 10093
    protocol: TCP
    targetPort: 10093
  selector:
    k8s-app: retina
---
# Source: retina-hubble/templates/hubble/peer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hubble-peer
  namespace: kube-system
  labels:
    k8s-app: retina
    app.kubernetes.io/part-of: retina
    app.kubernetes.io/name: hubble-peer
spec:
  selector:
    k8s-app: retina
  ports:
  - name: peer-service
    port: 80
    protocol: TCP
    targetPort: 4244
  internalTrafficPolicy: Local
---
# Source: retina-hubble/templates/agent/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: retina-agent
  namespace: kube-system
  labels:
    k8s-app: retina
spec:
  selector:
    matchLabels:
      app: retina
  template:
    metadata:
      labels:
        app: retina
        k8s-app: retina
      annotations:
        prometheus.io/port: "10093"
        prometheus.io/scrape: "true"
        checksum/config: cd2546b7063471ef39becafb96c4a828c32c5145dc727cac1705309419af190c
    spec:
      hostNetwork: true
      serviceAccountName: retina-agent
      nodeSelector:
        kubernetes.io/os: linux
      initContainers:
        - name: retina-agent-init
          image: ghcr.io/microsoft/retina/retina-init:v1.0.0-rc4
          imagePullPolicy: Always
          args:
            - --config
            - "/retina/config/config.yaml"
          terminationMessagePolicy: FallbackToLogsOnError
          securityContext:
            privileged: true
          volumeMounts:
          - name: bpf
            mountPath: /sys/fs/bpf
            mountPropagation: Bidirectional
          - name: cilium
            mountPath: /var/run/cilium
          - name: config
            mountPath: /retina/config
      containers:
        - name: retina 
          image: ghcr.io/microsoft/retina/retina-agent:v1.0.0-rc4
          imagePullPolicy: Always
          command:
          - /retina/controller
          args:
          - --health-probe-bind-address=:18081
          - --metrics-bind-address=:18080
          - "hubble-control-plane"
          - "--config-dir"
          - "/retina/config"
          ports:
          - containerPort: 10093
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "500Mi"
              cpu: "500m"
          readinessProbe:
            httpGet:
              path: /metrics
              port: 10093
            initialDelaySeconds: 10
            periodSeconds: 30
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: NODE_NAME
            valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
          - name: NODE_IP
            valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
          securityContext:
            capabilities:
              add:
              - SYS_ADMIN
              - SYS_RESOURCE
              - NET_ADMIN
              - IPC_LOCK
            privileged: false
          volumeMounts:
            - name: bpf
              mountPath: /sys/fs/bpf
            - name: cgroup
              mountPath: /sys/fs/cgroup
            - name: config
              mountPath: /retina/config
            - name: debug
              mountPath: /sys/kernel/debug
            - name: trace
              mountPath: /sys/kernel/tracing
            - name: cilium
              mountPath: /var/run/cilium
            - name: host-os-release
              mountPath: /etc/os-release
      terminationGracePeriodSeconds: 90 # Allow for retina to cleanup plugin resources.
      volumes:
      - name: bpf # Stores bpf maps
        hostPath:
          path: /sys/fs/bpf
      - name: cgroup # Mount point for cgroups
        hostPath:
          path: /sys/fs/cgroup
      - name: config # Stores retina configuration
        configMap:
          name: retina-config
      - name: debug # Stores data to trace kernel events
        hostPath:
          path: /sys/kernel/debug
      - name: trace # Stores data to trace kernel events
        hostPath:
          path: /sys/kernel/tracing
      - name: cilium # Stores hubble's socket path to listen for connections
        hostPath:
          path: /var/run/cilium
          type: DirectoryOrCreate
      - name: host-os-release
        hostPath:
          path: /etc/os-release
          type: FileOrCreate
---
# Source: retina-hubble/templates/hubble-relay/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hubble-relay
  namespace: kube-system
  labels:
    k8s-app: hubble-relay
    app.kubernetes.io/name: hubble-relay
    app.kubernetes.io/part-of: cilium
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-relay
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        k8s-app: hubble-relay
        app.kubernetes.io/name: hubble-relay
        app.kubernetes.io/part-of: cilium
    spec:
      securityContext:
        fsGroup: 65532
      containers:
        - name: hubble-relay
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          image: "mcr.microsoft.com/oss/cilium/hubble-relay:1.16.1"
          imagePullPolicy: Always
          command:
            - hubble-relay
          args:
            - serve
            - --disable-client-tls
            - --disable-server-tls
          ports:
            - name: grpc
              containerPort: 4245
          readinessProbe:
            grpc:
              port: 4245
            timeoutSeconds: 3
          livenessProbe:
            grpc:
              port: 4245
            timeoutSeconds: 3
          startupProbe:
            # give the relay one minute to start up
            grpc:
              port: 4245
            timeoutSeconds: 3
            failureThreshold: 20
            periodSeconds: 3
          volumeMounts:
          - name: config
            mountPath: /etc/hubble-relay
            readOnly: true
          terminationMessagePolicy: FallbackToLogsOnError
      restartPolicy: Always
      priorityClassName: 
      serviceAccount: "hubble-relay"
      serviceAccountName: "hubble-relay"
      automountServiceAccountToken: false
      terminationGracePeriodSeconds: 1
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                k8s-app: retina
            topologyKey: kubernetes.io/hostname
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
      - name: config
        configMap:
          name: hubble-relay-config
          items:
          - key: config.yaml
            path: config.yaml
---
# Source: retina-hubble/templates/hubble-ui/deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: hubble-ui
  namespace: kube-system
  labels:
    k8s-app: hubble-ui
    app.kubernetes.io/name: hubble-ui
    app.kubernetes.io/part-of: cilium
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: hubble-ui
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        k8s-app: hubble-ui
        app.kubernetes.io/name: hubble-ui
        app.kubernetes.io/part-of: cilium
    spec:
      priorityClassName: 
      serviceAccount: "hubble-ui"
      serviceAccountName: "hubble-ui"
      automountServiceAccountToken: true
      containers:
      - name: frontend
        image: "quay.io/cilium/hubble-ui:v0.13.3"
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8081
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
        readinessProbe:
          httpGet:
            path: /
            port: 8081
        volumeMounts:
        - name: hubble-ui-nginx-conf
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: nginx.conf
        - name: tmp-dir
          mountPath: /tmp
        terminationMessagePolicy: FallbackToLogsOnError
      - name: backend
        image: "quay.io/cilium/hubble-ui-backend:v0.13.3"
        imagePullPolicy: Always
        env:
        - name: EVENTS_SERVER_PORT
          value: "8090"
        - name: FLOWS_API_ADDR
          value: "hubble-relay:80"
        ports:
        - name: grpc
          containerPort: 8090
        volumeMounts:
        terminationMessagePolicy: FallbackToLogsOnError
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
      - configMap:
          defaultMode: 420
          name: hubble-ui-nginx
        name: hubble-ui-nginx-conf
      - emptyDir: {}
        name: tmp-dir
---
# Source: retina-hubble/templates/operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: retina-operator
  namespace: kube-system
  labels:
    app: retina-operator
    control-plane: retina-operator
    app.kubernetes.io/name: deployment
    app.kubernetes.io/instance: retina-operator
    app.kubernetes.io/component: retina-operator
    app.kubernetes.io/created-by: operator
    app.kubernetes.io/part-of: operator
    app.kubernetes.io/managed-by: kustomize
spec:
  selector:
    matchLabels:
      control-plane: retina-operator
  replicas: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: retina-operator
      labels:
        app: retina-operator
        control-plane: retina-operator
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      containers:
        - image: ghcr.io/microsoft/retina/retina-operator:v1.0.0-rc4
          imagePullPolicy: Always
          name: retina-operator
          command:
          - /retina-operator
          
          args:
          - "manage-cilium-crds"
          - "--config-dir"
          - "/retina"
          
          env:
          # this env var is used by retina OSS telemetry and zap
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          volumeMounts:
            - name: retina-operator-config
              mountPath: /retina/
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "ALL"
          # livenessProbe:
          #   httpGet:
          #     path: /healthz
          #     port: 8081
          #   initialDelaySeconds: 15
          #   periodSeconds: 20
          # readinessProbe:
          #   httpGet:
          #     path: /readyz
          #     port: 8081
          #   initialDelaySeconds: 5
          #   periodSeconds: 10
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 250m
              memory: 250Mi
      serviceAccountName: retina-operator
      terminationGracePeriodSeconds: 10
      volumes:
        - name: retina-operator-config
          configMap:
            name: retina-operator-config
---
# ServiceMonitor for Prometheus scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: retina-agent
  namespace: kube-system
  labels:
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      k8s-app: retina
  endpoints:
  - port: retina
    interval: 30s
    path: /metrics
---
# LoadBalancer for direct Retina metrics access
apiVersion: v1
kind: Service
metadata:
  name: retina-lb
  namespace: kube-system
  annotations:
    metallb.universe.tf/loadBalancerIPs: "192.168.178.102"
spec:
  type: LoadBalancer
  selector:
    k8s-app: retina
  ports:
  - name: metrics
    port: 10093
    targetPort: 10093
    protocol: TCP
